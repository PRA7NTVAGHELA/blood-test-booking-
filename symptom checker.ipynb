{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9258be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: flask in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn nltk flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95956875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Symptom2Disease_Final.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "df = df.dropna()# Ensure columns are correctly named\n",
    "df.columns = df.columns.str.strip() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9766b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1209 entries, 0 to 1208\n",
      "Data columns (total 11 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   disease                  1209 non-null   object\n",
      " 1   symptoms                 1209 non-null   object\n",
      " 2   age                      1209 non-null   object\n",
      " 3   gender                   1209 non-null   object\n",
      " 4   severity                 1209 non-null   object\n",
      " 5   duration                 1209 non-null   object\n",
      " 6   lifestyle                1209 non-null   object\n",
      " 7   family_history           1209 non-null   object\n",
      " 8   pre_existing_conditions  1209 non-null   object\n",
      " 9   medications              1209 non-null   object\n",
      " 10  recommended_tests        1209 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 104.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())  # Check for missing values and data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcf12f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts())  \u001b[38;5;66;03m# Check the distribution of diseases\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())  # Check the distribution of diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Preprocess the Data\n",
    "Malaria = df[df['label'] == 'Malaria']\n",
    "print(Malaria.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31a6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c3c6341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\ASUS/nltk_data', 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data', 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data', 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print(nltk.data.path)\n",
    "nltk.data.path.append('C:/Users/ASUS/nltk_data')  # Adjust to your path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2f05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning to the dataset\n",
    "df['cleaned_symptoms'] = df['symptoms'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to the dataset\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f4d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            symptoms  \\\n",
      "0  I have been experiencing a skin rash on my arm...   \n",
      "1  My skin has been peeling, especially on my kne...   \n",
      "2  I have been experiencing joint pain in my fing...   \n",
      "3  There is a silver like dusting on my skin, esp...   \n",
      "4  My nails have small dents or pits in them, and...   \n",
      "5  The skin on my palms and soles is thickened an...   \n",
      "6  The skin around my mouth, nose, and eyes is re...   \n",
      "7  My skin is very sensitive and reacts easily to...   \n",
      "8  I have noticed a sudden peeling of skin at dif...   \n",
      "9  The skin on my genitals is red and inflamed. I...   \n",
      "\n",
      "                                    cleaned_symptoms  \n",
      "0  experiencing skin rash arms legs torso past we...  \n",
      "1  skin peeling especially knees elbows scalp pee...  \n",
      "2  experiencing joint pain fingers wrists knees p...  \n",
      "3  silver like dusting skin especially lower back...  \n",
      "4  nails small dents pits often feel inflammatory...  \n",
      "5  skin palms soles thickened deep cracks cracks ...  \n",
      "6  skin around mouth nose eyes red inflamed often...  \n",
      "7  skin sensitive reacts easily changes temperatu...  \n",
      "8  noticed sudden peeling skin different parts bo...  \n",
      "9  skin genitals red inflamed often itchy burning...  \n"
     ]
    }
   ],
   "source": [
    "# Check a few examples of cleaned symptoms\n",
    "print(df[['symptoms', 'cleaned_symptoms']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eeb2419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 967\n",
      "Testing set size: 242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (Use \"cleaned_symptoms\" and \"disease\" instead of \"text\" and \"label\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_symptoms'], df['disease'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Testing set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Vectorize the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f767d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (967, 1387)\n",
      "Shape of testing data: (242, 1387)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Shape of training data:\", X_train_vec.shape)\n",
    "print(\"Shape of testing data:\", X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b016b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51fb5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9214876033057852\n",
      "Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                           Acne       1.00      1.00      1.00        11\n",
      "                      Arthritis       1.00      1.00      1.00         9\n",
      "               Bronchial Asthma       0.73      1.00      0.84         8\n",
      "           Cervical spondylosis       0.88      1.00      0.93         7\n",
      "                    Chicken pox       1.00      0.85      0.92        13\n",
      "                    Common Cold       0.92      0.92      0.92        12\n",
      "                         Dengue       0.92      0.85      0.88        13\n",
      "          Dimorphic Hemorrhoids       1.00      1.00      1.00        11\n",
      "               Fungal infection       1.00      1.00      1.00        10\n",
      "                   Hypertension       1.00      1.00      1.00         4\n",
      "                       Impetigo       1.00      1.00      1.00        11\n",
      "                       Jaundice       1.00      1.00      1.00         9\n",
      "                        Malaria       0.75      1.00      0.86         9\n",
      "                       Migraine       1.00      1.00      1.00        14\n",
      "                      Pneumonia       0.92      0.92      0.92        13\n",
      "                      Psoriasis       1.00      0.83      0.91         6\n",
      "                         Stroke       0.00      0.00      0.00         1\n",
      "                        Typhoid       0.89      0.89      0.89         9\n",
      "                 Varicose Veins       1.00      1.00      1.00        13\n",
      "                        allergy       1.00      0.75      0.86        12\n",
      "                       diabetes       0.86      0.86      0.86         7\n",
      "                  drug reaction       0.86      0.75      0.80         8\n",
      "gastroesophageal reflux disease       0.67      1.00      0.80        10\n",
      "           peptic ulcer disease       0.86      0.60      0.71        10\n",
      "        urinary tract infection       1.00      1.00      1.00        12\n",
      "\n",
      "                       accuracy                           0.92       242\n",
      "                      macro avg       0.89      0.89      0.88       242\n",
      "                   weighted avg       0.93      0.92      0.92       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize and train the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Get the confidence scores (probability)\n",
    "y_pred_proba = model.predict_proba(X_test_vec)\n",
    "\n",
    "# Convert probabilities to max confidence scores\n",
    "confidence_scores = y_pred_proba.max(axis=1)\n",
    "\n",
    "# Show some sample predictions\n",
    "for i in range(5):  # Print first 5 predictions\n",
    "    print(f\"Symptoms: {X_test.iloc[i]}\")\n",
    "    print(f\"Predicted Disease: {y_pred[i]}\")\n",
    "    print(f\"Confidence Score: {round(confidence_scores[i], 2)}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbfc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Save the Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eed193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'symptom_checker_model.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a0865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model and vectorizer\n",
    "model = joblib.load('symptom_checker_model.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Flask app\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db40b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b11f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    user_input = request.json['symptoms']\n",
    "\n",
    "    # Clean and process input\n",
    "    cleaned_input = clean_text(user_input)\n",
    "    input_vec = vectorizer.transform([cleaned_input])\n",
    "\n",
    "    # Get prediction and confidence\n",
    "    prediction_prob = model.predict_proba(input_vec)\n",
    "    predicted_label = model.classes_[prediction_prob.argmax()]\n",
    "    confidence = round(prediction_prob.max(), 2)\n",
    "\n",
    "    # Get additional details\n",
    "    disease_info = df[df['disease'] == predicted_label].iloc[0]\n",
    "    recommended_tests = eval(disease_info['recommended_tests'])  # Convert string to list\n",
    "    medications = eval(disease_info['medications'])  # Convert string to list\n",
    "\n",
    "    return jsonify({\n",
    "        \"predicted_disease\": predicted_label,\n",
    "        \"confidence\": confidence,\n",
    "        \"recommended_tests\": recommended_tests,\n",
    "        \"medications\": medications\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20018a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint\n",
    "url = 'http://127.0.0.1:5000/predict'\n",
    "\n",
    "# Define the input data\n",
    "data = {\n",
    "    'symptoms': 'There is bruising on my legs that I cannot explain. I can see strange blood vessels below the skin.'\n",
    "}\n",
    "\n",
    "# Send a POST request to the API\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f20b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188c9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
